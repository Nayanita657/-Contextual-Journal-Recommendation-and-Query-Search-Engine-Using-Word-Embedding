{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Query Search.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3jqf-fVppng",
        "outputId": "7036a04b-645c-4e06-e3ec-5b36f6ddba0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #to access drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWW6GgaYo07k",
        "outputId": "82db3f3c-f58e-4119-a6d9-71b33d015a07"
      },
      "source": [
        "!pip install mittens\n",
        "!pip install gingerit\n",
        "!pip install tika\n",
        "!pip install sentence-transformers "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n",
            "Requirement already satisfied: gingerit in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from gingerit) (2.26.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (1.24.3)\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZUpryaTW2M7",
        "outputId": "2709ca4e-0baa-494b-db28-7e658d723a2c"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from io import StringIO\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gingerit.gingerit import GingerIt\n",
        "# pip install tika\n",
        "from tika import parser\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0E1sNBIW2M-"
      },
      "source": [
        "def clean_document(page):\n",
        "    page = re.sub(r'[\\t]',' ',page)\n",
        "    page = re.sub(r'[\\n]',' ',page)\n",
        "    page = re.sub(r'[^.,a-zA-Z0-9 \\n\\.]',' ',page)\n",
        "    page = re.sub(r'\\s+',' ',page)\n",
        "    return page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp0ffBCVW2M-"
      },
      "source": [
        "import glob, os\n",
        "\n",
        "path = '/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/journals'\n",
        "\n",
        "def readfiles():\n",
        "    os.chdir(path)\n",
        "    pdfs = []\n",
        "    for file in glob.glob(\"*.pdf\"):\n",
        "        pdfs.append(file)\n",
        "    return pdfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAZIlkQeW2M_"
      },
      "source": [
        "def preprocess_text(document):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        "    document = re.sub(r'[^A-Za-z0-9]+',' ', document)\n",
        "\n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    #document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    #document = re.sub(r'^b\\s+', '', document)\n",
        "    #document = re.sub()\n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = document.split()\n",
        "    #tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in en_stop]\n",
        "    tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9cqU9cIW2NA"
      },
      "source": [
        "def sentance_correction(corpus):\n",
        "    textCorrected =[]\n",
        "    for i in corpus:\n",
        "        try:\n",
        "            parser = GingerIt()\n",
        "            text = parser.parse(i)['result']\n",
        "            textCorrected.append(text)\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    return textCorrected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijfFyeWKW2NB"
      },
      "source": [
        "files = readfiles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHsTpUbgW2NB",
        "outputId": "3bc98303-98b9-466b-e96b-c118df9532b2"
      },
      "source": [
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kundu-2015-a.pdf',\n",
              " 'An Analysis of Overlapping Community Detection Algorithms in Social Networks.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydxn_EqnW2NC",
        "outputId": "592991a3-1297-4334-a54e-9dea53bdc225"
      },
      "source": [
        "files = readfiles()\n",
        "\n",
        "df_all = []\n",
        "for file in files:\n",
        "    file_name = file #'ICDE_workshopv20.pdf'\n",
        "    raw = parser.from_file(file_name)\n",
        "    print(file)\n",
        "    corpus = [clean_document(raw['content'])] #word form\n",
        "    corpus_doc = [' '.join(corpus)] #sentence form\n",
        "\n",
        "    stemmer = WordNetLemmatizer()\n",
        "\n",
        "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    sentence = []\n",
        "    for s in corpus_doc:\n",
        "        sentence.append(sent_tokenize(s)) #paragraph\n",
        "\n",
        "\n",
        "    textCorrected = sentance_correction(sentence[0])\n",
        "    corpus = [preprocess_text(i) for i in textCorrected]\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['text'] = corpus\n",
        "    df['correct_sentances'] = textCorrected\n",
        "    df['file_name'] = file_name\n",
        "    file = df\n",
        "\n",
        "    df_all.append(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kundu-2015-a.pdf\n",
            "An Analysis of Overlapping Community Detection Algorithms in Social Networks.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcpT1Br0W2ND"
      },
      "source": [
        "df = pd.concat(df_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Xv_Rr_TJW2ND",
        "outputId": "c5fea481-62c1-4732-f2b1-20638ad68a84"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>correct_sentances</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pattern recognition letters journal homepage e...</td>\n",
              "      <td>1 Pattern Recognition Letters journal homepag...</td>\n",
              "      <td>kundu-2015-a.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>algorithms available literature mainly follow ...</td>\n",
              "      <td>The algorithms available in the literature mai...</td>\n",
              "      <td>kundu-2015-a.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>paper proposed novel community detection algor...</td>\n",
              "      <td>In this paper, we proposed a novel community d...</td>\n",
              "      <td>kundu-2015-a.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>algorithm runs framework social network repres...</td>\n",
              "      <td>The algorithm runs on a new framework of socia...</td>\n",
              "      <td>kundu-2015-a.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>index</td>\n",
              "      <td>A new index viz.</td>\n",
              "      <td>kundu-2015-a.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>2007</td>\n",
              "      <td>036 106, 2007 .</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>lancichinetti fortunato kertesz detecting over...</td>\n",
              "      <td>24 A. Lancichinetti, S. Fortunato and J. Kerte...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>033015 2009</td>\n",
              "      <td>11, 033015, p. 18, 2009.</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>qiguo maozu yang xiaoyan ling chen mlpa detect...</td>\n",
              "      <td>25 Qiguo Dai, MaoZu Guo, Yang Liu, Xiaoyan Liu...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>ting wang qian xiaomeng wang hybrid label prop...</td>\n",
              "      <td>26 Ting Wang, Xu Qian and Xiaomeng Wang, A Hyb...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>729 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...                                          file_name\n",
              "0    pattern recognition letters journal homepage e...  ...                                   kundu-2015-a.pdf\n",
              "1    algorithms available literature mainly follow ...  ...                                   kundu-2015-a.pdf\n",
              "2    paper proposed novel community detection algor...  ...                                   kundu-2015-a.pdf\n",
              "3    algorithm runs framework social network repres...  ...                                   kundu-2015-a.pdf\n",
              "4                                                index  ...                                   kundu-2015-a.pdf\n",
              "..                                                 ...  ...                                                ...\n",
              "346                                               2007  ...  An Analysis of Overlapping Community Detection...\n",
              "347  lancichinetti fortunato kertesz detecting over...  ...  An Analysis of Overlapping Community Detection...\n",
              "348                                        033015 2009  ...  An Analysis of Overlapping Community Detection...\n",
              "349  qiguo maozu yang xiaoyan ling chen mlpa detect...  ...  An Analysis of Overlapping Community Detection...\n",
              "350  ting wang qian xiaomeng wang hybrid label prop...  ...  An Analysis of Overlapping Community Detection...\n",
              "\n",
              "[729 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGbrocQTW2NE",
        "outputId": "5feb076e-cb10-439b-ac1a-99ebf2bdfff0"
      },
      "source": [
        "query = input('enter the query:')\n",
        "query = preprocess_text(query)\n",
        "parser = GingerIt()\n",
        "query = parser.parse(query)['result']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter the query:modularity is in the community detction?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jv-0VX1RW2NE",
        "outputId": "935e36dc-4055-43ad-9d7f-6ec22f50a28d"
      },
      "source": [
        "query"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'modularity community detection'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF9sDcsVrc6F",
        "outputId": "0e099619-d967-4ccb-c6bc-1b2e038d9b7b"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation')\n",
        "\n",
        "from Embedding_models import glove50_embedding\n",
        "from Embedding_models import Bert_embedding\n",
        "from Embedding_models import elmo_embedding\n",
        "\n",
        "glove_model = glove50_embedding(df)\n",
        "glove_model.fit()\n",
        "\n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(glove_model, '/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/glove_trained.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/glove_trained.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP379-S0rjCb",
        "outputId": "74412d19-e2f1-4da3-97f0-11e20be83435"
      },
      "source": [
        "# Load the model from the file\n",
        "glove_model_saved = joblib.load('/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/glove_trained.pkl')\n",
        "\n",
        "df_glove = glove_model_saved.fit_transform(query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/Embedding_models.py:112: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  cosine_similarity = nominator / denominator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09qrWZvSdjnt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xabmJuhsrnc8",
        "outputId": "84138554-4456-4c19-bee9-e4dce7571931"
      },
      "source": [
        "df_glove.iloc[:5,1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correct_sentances</th>\n",
              "      <th>file_name</th>\n",
              "      <th>cosine_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Modularity Approaches in Community Detection V...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.877165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Overview of Modularity Adaptation based Commun...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.768273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It gives the review of modularity adaptation b...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.738082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Section 3 explains the modularity based commun...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.737236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This work aims at providing the characteristic...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.727032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   correct_sentances  ... cosine_score\n",
              "0  Modularity Approaches in Community Detection V...  ...     0.877165\n",
              "1  Overview of Modularity Adaptation based Commun...  ...     0.768273\n",
              "2  It gives the review of modularity adaptation b...  ...     0.738082\n",
              "3  Section 3 explains the modularity based commun...  ...     0.737236\n",
              "4  This work aims at providing the characteristic...  ...     0.727032\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMj4MGb0ue7D",
        "outputId": "68842028-9981-467f-a3be-f2b9c4c8a78b"
      },
      "source": [
        "list(df_glove['cosine_score'].head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8771647214889526,\n",
              " 0.7682733535766602,\n",
              " 0.7380821108818054,\n",
              " 0.7372363805770874,\n",
              " 0.7270323634147644,\n",
              " 0.6977030634880066,\n",
              " 0.6686495246921579,\n",
              " 0.6577390432357788,\n",
              " 0.6492091417312622,\n",
              " 0.639761209487915]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8Rs7iBHW2NF",
        "outputId": "5761295f-2432-41d2-95bd-8cc23f21e8b3"
      },
      "source": [
        "bert_model = Bert_embedding(df)\n",
        "bert_model.fit()\n",
        "\n",
        " \n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(bert_model, '/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/bert_trained.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/bert_trained.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqK1CtfrW2NF"
      },
      "source": [
        "# Load the model from the file\n",
        "bert_model_saved = joblib.load('/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/bert_trained.pkl')\n",
        "\n",
        "df_bert = bert_model_saved.fit_transform(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0KtrvIn4W2NG",
        "outputId": "57325b1e-3296-4143-b72e-0d8becc657cd"
      },
      "source": [
        "df_bert.iloc[:5,1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correct_sentances</th>\n",
              "      <th>file_name</th>\n",
              "      <th>cosine_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overview of Modularity Adaptation based Commun...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.931679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Section 3 explains the modularity based commun...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.899921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It gives the review of modularity adaptation b...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.848959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This modularity is the strength of partition o...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.845770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Modularity defines the strength of partition o...</td>\n",
              "      <td>An Analysis of Overlapping Community Detection...</td>\n",
              "      <td>0.839639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   correct_sentances  ... cosine_score\n",
              "0  Overview of Modularity Adaptation based Commun...  ...     0.931679\n",
              "1  Section 3 explains the modularity based commun...  ...     0.899921\n",
              "2  It gives the review of modularity adaptation b...  ...     0.848959\n",
              "3  This modularity is the strength of partition o...  ...     0.845770\n",
              "4  Modularity defines the strength of partition o...  ...     0.839639\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyApUasJW2NG",
        "outputId": "771f9dca-bc79-4d84-cf35-7aa211b26884"
      },
      "source": [
        "list(df_bert['correct_sentances'].head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Overview of Modularity Adaptation based Community Detection.',\n",
              " 'Section 3 explains the modularity based community detection approach in detail.',\n",
              " 'It gives the review of modularity adaptation based community detection algorithms.',\n",
              " 'This modularity is the strength of partition of the network as a community.',\n",
              " 'Modularity defines the strength of partition of network into communities.',\n",
              " 'Modularity Approaches in Community Detection Various algorithms of community detection are analyzed based on modularity.',\n",
              " 'The modularity value of the community structure formed in the Fig.',\n",
              " 'Definition 8 Community.',\n",
              " 'Summary of Modularity Score.',\n",
              " 'It is used in modularity function to find the strength of community.']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6v6xj2hW2NJ",
        "outputId": "d94190db-9990-4b51-e424-bba37aea81a5"
      },
      "source": [
        "from Embedding_models import elmo_embedding\n",
        "\n",
        "elmo_model = elmo_embedding(df)\n",
        "elmo_model.fit()\n",
        "\n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(glove_model, '/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/elmo_trained.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnMzXxWkW2NJ"
      },
      "source": [
        "# Load the model from the file\n",
        "elmo_model_saved = joblib.load('/content/drive/MyDrive/Classroom/2021 Machine Learning 1 (CSL7550)/Project/Implementation/elmo_trained.pkl')\n",
        "\n",
        "df_elmo = elmo_model_saved.fit_transform(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2f1IRxOW2NJ"
      },
      "source": [
        "df_elmo.iloc[:5,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S2AXuc5W2NK"
      },
      "source": [
        "list(df_elmo['correct_sentances'].head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw2I1sqyW2NL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}